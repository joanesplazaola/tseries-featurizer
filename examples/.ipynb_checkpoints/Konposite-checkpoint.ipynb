{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import itertools\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/media/joanes/0BB3-1FA1/CSV_DATA/'\n",
    "files = glob(f'{PATH}*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def rf_feat_importance(m, df):\n",
    "    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n",
    "                       ).sort_values('imp', ascending=False)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    \n",
    "    accuracy = m.score(test_features, test_labels)\n",
    "    mse = mean_squared_error(test_labels, model.predict(test_features))\n",
    "    \n",
    "    print('Model Performance')\n",
    "    print('Accuracy = {:0.4f}%.'.format(accuracy))\n",
    "    print('Mean Squared Error = {:0.4f}%.'.format(mse))\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_list_and_target(files):\n",
    "    # Get only those values that are available before the analysis\n",
    "    filter_cols = ['Total_UnfilledZones', 'Total_FillingQuality', 'TOTAL_QUALITY',  'Total_PorosityQuantity', 'Total_PorosityQuality', 'Time']\n",
    "    df_list = list()\n",
    "    target = list()\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        target.append(df.TOTAL_QUALITY.unique()[0])\n",
    "        df_filtered = df.drop(axis=1, columns=filter_cols)\n",
    "        filter_col = [col for col in df_filtered if not col.endswith(('VoidContent','VoidQuality', 'Filling', 'FillingQuality'))]\n",
    "        df_filtered = df_filtered[filter_col]\n",
    "        df_list.append(df_filtered)\n",
    "\n",
    "    target = pd.DataFrame(target, columns=['valid'])\n",
    "    return df_list, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# model_ratio = 0.02\n",
    "# model_size = int(len(files) * model_ratio)\n",
    "\n",
    "# df_list, target_model = get_df_list_and_target(files[:model_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add library's path to notebook\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append('../../time-series-featurizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from ts_featurizer import TimeSeriesFeaturizer\n",
    "\n",
    "# tseries = TimeSeriesFeaturizer()\n",
    "# model = tseries.featurize(df_list, n_jobs=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filehandler = open('tmp/tseries.pickle', 'wb') \n",
    "# pickle.dump(tseries, filehandler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filehandler = open('tmp/tseries.pickle', 'rb') \n",
    "tseries = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tseries._featurized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------------------------------------------- Applying the model started --------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "train_ratio = 0.1\n",
    "train_size = int(len(files) * train_ratio)\n",
    "for time in range(1, 7):\n",
    "    start = train_size  * time\n",
    "    df_list, target_featurized = get_df_list_and_target(files[start: start + train_size])\n",
    "    featurized = tseries.featurize(df_list, n_jobs=7, apply_model=True)\n",
    "    featurized.reset_index(drop=True).to_feather(f'tmp/featurized_{time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.concat([model, featurized]) # It's important to do it in the original order\n",
    "target = pd.concat([target_model, target_featurized])\n",
    "print(features.dtypes.unique())\n",
    "print(f'NaN features: {features.isna().sum().sum()}')\n",
    "na_cols = features.columns[features.isna().any()].tolist()\n",
    "features.drop(axis=1, columns=na_cols, inplace=True)\n",
    "print(f'NaN features: {features.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(df, n): return df[:n], df[n:]\n",
    "\n",
    "\n",
    "X_train, X_test = split_vals(features, train_size)\n",
    "y_train, y_test = split_vals(target, train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store featurized dataframe to feather files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "X_test.reset_index(drop=True).to_feather('tmp/X_test')\n",
    "X_train.reset_index(drop=True).to_feather('tmp/X_train')\n",
    "y_train.reset_index(drop=True).to_feather('tmp/y_train')\n",
    "y_test.reset_index(drop=True).to_feather('tmp/y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read feather file into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import feather\n",
    "\n",
    "# X_test = feather.read_dataframe('tmp/X_test')\n",
    "# X_train = feather.read_dataframe('tmp/X_train')\n",
    "# y_train = feather.read_dataframe('tmp/y_train')\n",
    "# y_test = feather.read_dataframe('tmp/y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = RandomForestClassifier(n_estimators=80,max_features=0.5, min_samples_leaf=10, oob_score=True, n_jobs=-1)\n",
    "# m.fit(X_train, y_train.values.ravel())\n",
    "# print(f'Training score: {m.score(X_train, y_train.values.ravel())}')\n",
    "# print(f'Testing score: {m.score(X_test, y_test.values.ravel())}')\n",
    "# print(f'Out of bag score: {m.oob_score_}')\n",
    "# print('\\n')\n",
    "# evaluate(m, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = m.predict(X_test)\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cnf_matrix = confusion_matrix(y_pred, y_test)\n",
    "\n",
    "# # Plot non-normalized confusion matrix\n",
    "\n",
    "# plot_confusion_matrix(cnf_matrix, classes=[ 'Desegokia', 'Egokia'], title='Normalizatu gabeko konfusio matrizea')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get initial classifier's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = RandomForestClassifier(n_estimators=80,max_features=0.5, min_samples_leaf=10, oob_score=True, n_jobs=-1)\n",
    "# m.fit(train, target[:train_size].values.ravel())\n",
    "# print(f'Training score: {m.score(train, target[:train_size].values.ravel())}')\n",
    "# print(f'Testing score: {m.score(test, target[train_size:].values.ravel())}')\n",
    "# print(f'Out of bag score: {m.oob_score_}')\n",
    "# print('\\n')\n",
    "# evaluate(m, test, target[train_size:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get classifier's confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# y_test = m.predict(test)\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cnf_matrix = confusion_matrix(y_test, target[train_size:])\n",
    "\n",
    "# # Plot non-normalized confusion matrix\n",
    "\n",
    "# plot_confusion_matrix(cnf_matrix, classes=[ 'Desegokia', 'Egokia'], title='Normalizatu gabeko konfusio matrizea')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw = pd.concat([train, test])\n",
    "# df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fi = rf_feat_importance(df=train, m=m)\n",
    "\n",
    "# fi[:50].plot('cols', 'imp', 'barh', figsize=(20,7))\n",
    "# plt.show()\n",
    "\n",
    "# df_important = df_raw[fi[:50].cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check redundancy between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from scipy.cluster import hierarchy as hc\n",
    "\n",
    "# corr = np.round(scipy.stats.spearmanr(df_important).correlation, 4)\n",
    "# corr_condensed = hc.distance.squareform(1-corr)\n",
    "# z = hc.linkage(corr_condensed, method='average')\n",
    "# fig = plt.figure(figsize=(16,10))\n",
    "# dendrogram = hc.dendrogram(z, labels=df_important.columns, orientation='left', leaf_font_size=16)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can see that many important features are very highly correlated, so we will try to run the classifier without those who are redundant (only deleting one of both redundant cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundants = ['Zone12_Pressure_Time_numpy.median', 'Zone16_Pressure_Time_scipy.stats.kurtosis', 'Zone24_Pressure_Time_scipy.stats.kurtosis',\n",
    "#             'Zone24_tfilling_Time_numpy.median','Zone24_tfilling_Time_numpy.min', 'Zone24_tfilling_Time_numpy.mean',\n",
    "#              'Zone24_tfilling_Time_numpy.max', 'Zone24_tfilling_Time_numpy.percentile_95']\n",
    "\n",
    "\n",
    "# for red in redundants:\n",
    "#     m = RandomForestClassifier(n_estimators=80,max_features=0.5, min_samples_leaf=10, oob_score=True, n_jobs=-1)\n",
    "#     wo_red = df_important.drop(axis=1, columns=[red])\n",
    "#     m.fit(wo_red[:train_size], target[:train_size].values.ravel())\n",
    "#     print(f'{red}:\\n')\n",
    "#     print(f'\\tTraining score: {m.score(wo_red[:train_size], target[:train_size].values.ravel())}')\n",
    "#     print(f'\\tTesting score: {m.score(wo_red[train_size:], target[train_size:].values.ravel())}')\n",
    "#     print(f'\\tOut of bag score: {m.oob_score_}\\n')\n",
    "\n",
    "# m = RandomForestClassifier(n_estimators=80,max_features=0.5, min_samples_leaf=10, oob_score=True, n_jobs=-1)\n",
    "# df_wo_redundant = df_important.drop(axis=1, columns=redundants)\n",
    "# m.fit(df_wo_redundant[:train_size], target[:train_size].values.ravel())\n",
    "\n",
    "# print(f'Without redundants:\\n')\n",
    "# print(f'\\tTraining score: {m.score(df_wo_redundant[:train_size], target[:train_size].values.ravel())}')\n",
    "# print(f'\\tTesting score: {m.score(df_wo_redundant[train_size:], target[train_size:].values.ravel())}')\n",
    "# print(f'\\tOut of bag score: {m.oob_score_}\\n')\n",
    "\n",
    "# base_accuracy = evaluate(m, df_wo_redundant[train_size:], target[train_size:].values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that without those redundant features, our model is still quite solid, so we remove them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch in order to calculate best hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "#     # Get Test Scores Mean and std for each grid search\n",
    "#     scores_mean = cv_results['mean_test_score']\n",
    "#     scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "#     scores_sd = cv_results['std_test_score']\n",
    "#     scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "#     # Plot Grid search scores\n",
    "#     figure, ax = plt.subplots(1,1, figsize=(15,6))\n",
    "\n",
    "#     # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "#     for idx, val in enumerate(grid_param_2):\n",
    "#         ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "#     ax.set_title(\"Grid Search Scores\", fontsize=18, fontweight='bold')\n",
    "#     ax.set_xlabel(name_param_1, fontsize=16)\n",
    "#     ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "#     ax.legend(loc=\"best\", fontsize=10)\n",
    "#     ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = [80, 100, 160, 200]\n",
    "# max_features = [0.5, 'log2', 'sqrt', 'auto']\n",
    "# grid_search = GridSearchCV(m,\n",
    "#             dict(n_estimators=n_estimators,\n",
    "#                  max_features=max_features),\n",
    "#                  cv=5, n_jobs=8)\n",
    "\n",
    "# grid_search.fit(features[:train_size], target[:train_size].values.ravel());\n",
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_grid_search(grid_search.cv_results_, n_estimators, max_features, 'N Estimators', 'Max Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_grid = grid_search.best_estimator_\n",
    "# grid_accuracy = evaluate(best_grid, features[train_size:], target[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we see how the score of the max_features=0.5 is the best, and is quite constant when it comes to n_estimators. Now, we choose max_features=0.5, and we try to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [80, 90, 100, 110],\n",
    "#     'max_features': [0.5],\n",
    "#     'min_samples_leaf': [3, 4, 5],\n",
    "#     'min_samples_split': [8, 10, 12],\n",
    "#     'n_estimators': [100, 200, 300]\n",
    "# }\n",
    "# clf = RandomForestClassifier()\n",
    "\n",
    "# pipe_grid = GridSearchCV(estimator=clf, param_grid=param_grid, cv=4, n_jobs=-1, verbose=2)\n",
    "\n",
    "# pipe_grid.fit(features[:train_size], target[:train_size].values.ravel());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_grid = pipe_grid.best_estimator_\n",
    "# grid_accuracy = evaluate(best_grid, features[train_size:], target[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We check the how the feature importance is now and the redundat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fi = rf_feat_importance(df=features[:train_size], m=best_grid)\n",
    "\n",
    "# fi[:60].plot('cols', 'imp', 'barh', figsize=(20,7))\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.cluster import hierarchy as hc\n",
    "\n",
    "# corr = np.round(scipy.stats.spearmanr(df_wo_redundant).correlation, 4)\n",
    "# corr_condensed = hc.distance.squareform(1-corr)\n",
    "# z = hc.linkage(corr_condensed, method='average')\n",
    "# fig = plt.figure(figsize=(16,10))\n",
    "# dendrogram = hc.dendrogram(z, labels=df_wo_redundant.columns, orientation='left', leaf_font_size=16)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
